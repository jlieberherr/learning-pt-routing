{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "# Filter gtfs-feed\n",
    "A notebook with which timetables from a gtfs-feed can be filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import codecs\n",
    "from zipfile import ZipFile\n",
    "import tempfile\n",
    "import shutil\n",
    "import requests\n",
    "import io\n",
    "import glob\n",
    "from xlsxwriter.workbook import Workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\dev\\\\learning-pt-routing\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# url to the input gtfs-feed\n",
    "url_to_gtfs_feed = \"https://opentransportdata.swiss/dataset/6f55f96d-7644-4901-b927-e9cf05a8c7f0/resource/a81c59c2-6fd7-47c8-b7b6-90a045a90aae/download/gtfsfp20202020-01-22.zip\"\n",
    "\n",
    "# folder and file-name for the filtered gtfs-feed\n",
    "output_gtfs_small_dir = os.path.join(\"..\", \"resources\")\n",
    "output_gtfs_small_file_name = \"small_gtfs_feed\"  # .zip extension is added automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Filter gtfs-feed\n",
    "Given a list of routes we filter all information in the input-gtfs-feed which is connected to one of these routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_gtfs_feed = io.BytesIO(requests.get(url_to_gtfs_feed).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# define the routes\n",
    "route_id_bus_10_bern = \"6-10-j20-1\"\n",
    "route_id_ic_geneve_stgallen = \"26-1-A-j20-1\"\n",
    "route_id_chur_stmoritz = \"59-9-Y-j20-1\"\n",
    "route_ids = [route_id_bus_10_bern, route_id_ic_geneve_stgallen, route_id_chur_stmoritz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def filter_gtfs_feed(input_gtfs_feed_io, output_gtfs_feed_dir, output_gtfs_feed_filename, routes_to_filter):\n",
    "    \"\"\"filters the gtfs-data in input_gtfs_feed_io connected to the routes in routes_to_filter. returns a gtfs-zip- and an excel-file with the filtered data.\"\"\"\n",
    "    # create temporary directory\n",
    "    path_tmp_dict = os.path.join(output_gtfs_feed_dir, \"tmp\")\n",
    "    if os.path.exists(path_tmp_dict) and os.path.isdir(path_tmp_dict):\n",
    "        shutil.rmtree(path_tmp_dict)\n",
    "    os.mkdir(path_tmp_dict)\n",
    "    \n",
    "    def filter_by_id(key, values, zip_file, feed_file, new_keys, out_folder):\n",
    "        print(\"start processing {}\".format(feed_file))\n",
    "        \"\"\"filters the rows from feed_file if the value of the field key is contained in vlaues (all other rows are skipped)\"\"\"\n",
    "        with zip_file.open(feed_file, \"r\") as gtfs_file:\n",
    "            reader = csv.DictReader(io.TextIOWrapper(gtfs_file, \"utf-8-sig\"))\n",
    "            res = []\n",
    "            new_values = []\n",
    "            for l in reader:\n",
    "                if l[key] in values:\n",
    "                    res += [l]\n",
    "                    new_values += [[l[new_key] for new_key in new_keys]]\n",
    "            with open(os.path.join(out_folder, feed_file), \"w\", newline='', encoding=\"utf8\") as g:\n",
    "                writer = csv.DictWriter(g, fieldnames=reader.fieldnames, quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "                writer.writeheader()\n",
    "                for l in res:\n",
    "                    writer.writerow(l)\n",
    "        print(\"end processing {}\".format(feed_file))\n",
    "        return [set(x) for x in zip(*new_values)]\n",
    "    \n",
    "\n",
    "    def filter_transfers(stop_ids, zip_file, out_folder):\n",
    "        \"\"\"filters the rows from transfers.txt if from_stop_id and to_stop_id is are contained in stop_ids (all other rows are skipped)\"\"\"\n",
    "        print(\"start processing transfers.txt\")\n",
    "        with zip_file.open(\"transfers.txt\", \"r\") as gtfs_file:\n",
    "            reader = csv.DictReader(io.TextIOWrapper(gtfs_file, \"utf-8-sig\"))\n",
    "            res = []\n",
    "            for l in reader:\n",
    "                if l[\"from_stop_id\"] in stop_ids and l[\"to_stop_id\"] in stop_ids:\n",
    "                    res += [l]\n",
    "            with open(os.path.join(out_folder, \"transfers.txt\"), \"w\", newline='', encoding=\"utf8\") as g:\n",
    "                writer = csv.DictWriter(g, fieldnames=reader.fieldnames, quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "                writer.writeheader()\n",
    "                for l in res:\n",
    "                    writer.writerow(l)\n",
    "        print(\"end processing transfers.txt\")\n",
    "    \n",
    "    def add_parent_stations_to_stop_ids(stop_ids, zip_file):\n",
    "        \"\"\"adds the ids of the parent stations to the stop-ids in stop_ids\"\"\" \n",
    "        print(\"start processing parent stops\")\n",
    "        parent_stations_to_add = set()\n",
    "        with zip_file.open(\"stops.txt\", \"r\") as gtfs_file:\n",
    "            reader = csv.DictReader(io.TextIOWrapper(gtfs_file, \"utf-8-sig\"))\n",
    "            for l in reader:\n",
    "                if l.get(\"stop_id\") in stop_ids and l.get(\"parent_station\", None):\n",
    "                    parent_stations_to_add.add(l[\"parent_station\"])\n",
    "        print(\"end processing parent stops\")\n",
    "        return stop_ids.union(parent_stations_to_add)\n",
    "    \n",
    "    # process step by step\n",
    "    with ZipFile(io_gtfs_feed, \"r\") as zip_file:\n",
    "        trip_ids, service_ids = filter_by_id(\"route_id\", route_ids, zip_file, \"trips.txt\", [\"trip_id\", \"service_id\"], path_tmp_dict)\n",
    "        stop_ids = filter_by_id(\"trip_id\", trip_ids, zip_file, \"stop_times.txt\", [\"stop_id\"], path_tmp_dict)[0]\n",
    "        stop_ids = add_parent_stations_to_stop_ids(stop_ids, zip_file)\n",
    "        filter_by_id(\"stop_id\", stop_ids, zip_file, \"stops.txt\", [], path_tmp_dict)\n",
    "        filter_by_id(\"service_id\", service_ids, zip_file, \"calendar.txt\", [], path_tmp_dict)\n",
    "        filter_by_id(\"service_id\", service_ids, zip_file, \"calendar_dates.txt\", [], path_tmp_dict)\n",
    "        agency_ids = filter_by_id(\"route_id\", route_ids, zip_file, \"routes.txt\", [\"agency_id\"], path_tmp_dict)[0]\n",
    "        filter_by_id(\"agency_id\", agency_ids, zip_file, \"agency.txt\", [], path_tmp_dict)\n",
    "        filter_transfers(stop_ids, zip_file, path_tmp_dict)\n",
    "   \n",
    "    # zip the text-files to a zip-file\n",
    "    shutil.make_archive(os.path.join(output_gtfs_feed_dir, output_gtfs_feed_filename), \"zip\", path_tmp_dict)\n",
    "    \n",
    "    # write the text-files to a excel-file\n",
    "    workbook = Workbook(os.path.join(output_gtfs_feed_dir, \"{}.xlsx\".format(output_gtfs_small_file_name)))\n",
    "    for csvfile in glob.glob(os.path.join(output_gtfs_feed_dir, \"tmp\", '*.txt')):\n",
    "        worksheet = workbook.add_worksheet(os.path.basename(csvfile)[:-4])\n",
    "        with open(csvfile, 'rt', encoding='utf8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for r, row in enumerate(reader):\n",
    "                for c, col in enumerate(row):\n",
    "                    worksheet.write(r, c, col)\n",
    "    workbook.close()\n",
    "    \n",
    "    # remove tmp-data\n",
    "    if os.path.exists(path_tmp_dict) and os.path.isdir(path_tmp_dict):\n",
    "        shutil.rmtree(path_tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start processing trips.txt\n",
      "end processing trips.txt\n",
      "start processing stop_times.txt\n",
      "end processing stop_times.txt\n",
      "start processing parent stops\n",
      "end processing parent stops\n",
      "start processing stops.txt\n",
      "end processing stops.txt\n",
      "start processing calendar.txt\n",
      "end processing calendar.txt\n",
      "start processing calendar_dates.txt\n",
      "end processing calendar_dates.txt\n",
      "start processing routes.txt\n",
      "end processing routes.txt\n",
      "start processing agency.txt\n",
      "end processing agency.txt\n",
      "start processing transfers.txt\n",
      "end processing transfers.txt\n"
     ]
    }
   ],
   "source": [
    "filter_gtfs_feed(io_gtfs_feed, output_gtfs_small_dir, output_gtfs_small_file_name, route_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
